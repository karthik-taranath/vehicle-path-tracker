# Vehicle Path Tracking

## Introduction
This project calculates the visual odometry of a car driving through a neighborhood. It takes a series of pictures with small amounts of motion as an input and uses this to predict the path that it took during the time elapsed in these frames. This process also requires a prior knowledge of the intrinsic parameters of the camera.

## Computing the Intrinsic Matrix
The intrinsic matrix is the 3x3 matrix that contains the information about the focal length and principle point of the camera. This is needed to recover the pose from the essential matrix, but it can also be used to optionally undistort the images. It is read in from a provided file that contained these details, and then can be constructed from these values manually.

## Loading and Demosaicing Images
The images I am using are in the Bayer format. This is a special format where the image is sectioned into 2x2 image blocks, each containing two green pixels, one blue pixel, and one red pixel. This is why including the flag -1 is important, as it preserves all of the channels. This then allows the image to be demosaiced, which combines the colors into an RGB image. The image is then undistorted using a lookup table generated by information about the camera. Once it has been fully processed, it is added to an array containing all of the images.

## Keypoint Matching
Once the images are all loaded, then each image and the image that follows are processed in a loop to calculate the transformations in between each frame. The first step is to match features between each pair of images. This is done through the SIFT function provided by OpenCV, which identifes interesting features at different scales through Gaussian pyramids, assigns orientations to each feature, and creating descriptors to represent the image around them. To match the keypoints in each picture to each other, I used a Fast Library for Approximate Nearest Neighbors (FLANN) matcher provided by OpenCV. This works by using a KD-tree to quickly identify nearest neighbors using Hamming distance, which is a way of comparing binary information. Lastly, to filter out bad matches, I used Loweâ€™s ratio test to filter out matches whose nearest neighbors were were not significantly closer than the next nearest neighbors by comparing them to a certain threshold, which I chose to be 0.5.

## Estimating the Fundamental Matrix
The fundamental matrix is the 3x3 matrix that relates two different views of the same scene together. It is calculated using keypoint matches and can be used to calculate the epipolar lines that ultimately represent the perspective shift from one image to the other. This is done by using the keypoint matches that were calculated in the previous section and the fundamental matrix calculation function provided by OpenCV. This works through the RANSAC algorithm. This randomly samples sets of seven points from a large pool and uses the seven point algorithm to calculate the fundamental matrix, choosing the sample with the largest number of inliers. The seven point algorithm takes advantage of the fact that there are seven degrees of freedom when calculating the fundamental matrix, so it creates a linear system of equations and uses the least squares solution.

## Recovering the Essential Matrix
The essential matrix, like the fundamental matrix, also represents the relationship between two viewpoints of the same scene. This matrix is different, as it instead contains information about the rotation and translation from one scene to the other. In order to do this, the intrinsic parameters of the camera are once again needed. This is because the information in the fundamental matrix is relative to each picture. By introducing the calibration of the cameras, the absolute difference between the two viewpoints can be calculated.

## Reconstructing Rotation and Translation Parameters
The transformation matrix is the 4x4 matrix that directly contains how to get from one frame to the next. The top three rows and columns represents the rotation matrix and the fourth column and first three rows represents the translation matrix. To get this from the essential matrix, the pose recovery function from OpenCV is used. This works by decomposing the essential matrices into the camera center and rotation matrix. There are four possible combinations of these matrices that can be made. The correct one is chosen through by checking the cheirality condition, which ensures that the points are in front of the camera. This then returns the correct rotation and translation matrices that can be constructed into the final transformation matrix from one step to the next.

## Reconstructing the Trajectory
The initial position of the car starts at the origin, which is represented by the 4x4 identity matrix. This says that the orientation is forward in all directions and the position is zero. Then, the position at each frame can be calculated by multiplying the previous position by the transformation matrix. By building off each previous frame, the entire path of the car can be made, which is has been correctly created, as shown in the final 2D and 3D graphs below.